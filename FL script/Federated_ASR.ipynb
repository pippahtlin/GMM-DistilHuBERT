{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e998eaf8-5779-4744-931c-fd950d7b9d31",
   "metadata": {},
   "source": [
    "### Federated Fine-tuning for ASR\n",
    "* Centralized: You fine-tune DistilHuBERT for downstream ASR tasks.\n",
    "* FL: Simulate a federated ASR learning scenario where multiple clients (speakers) fine-tune a shared model on local speech data using FedAvg/FedOpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddaa46a2-3144-4aa0-a272-c6642604ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7964eb38-e7d0-449f-8ffe-461f24b0d74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1568f8cac80b41ccab9937c6c9bb2a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/scratch/pippalin2/jupyter/GMM-DistilHuBERT/processed_dataset\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c2ee39-91c7-4088-9703-b19ab2c6cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 25670\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abe893-61b2-492e-b44c-66f63cff3928",
   "metadata": {},
   "source": [
    "#### Simulating FL setting: clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cace8a-ae84-416d-b339-d668d90fda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_clients_nonuniform(dataset, num_clients, min_frac, max_frac):\n",
    "    size = len(dataset)\n",
    "\n",
    "    proportions = np.random.uniform(min_frac, max_frac, size=num_clients)\n",
    "    proportions = proportions / proportions.sum()\n",
    "    sizes = (proportions * size).astype(int)\n",
    "\n",
    "    # Ensure total size matches\n",
    "    diff = size - sizes.sum()\n",
    "    sizes[0] += diff\n",
    "\n",
    "    # Client splits\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    client_datasets = []\n",
    "    start = 0\n",
    "    for s in sizes:\n",
    "        end = start + s\n",
    "        client_datasets.append(dataset.select(indices[start:end].tolist()))\n",
    "        start = end\n",
    "\n",
    "    return client_datasets\n",
    "client_datasets = split_into_clients_nonuniform(train_dataset, num_clients=20, min_frac=0.01, max_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b7a2b-edf1-40e8-99dc-7a3bdfbc0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"/scratch/pippalin2/jupyter/GMM-DistilHuBERT/processor\")\n",
    "base_model = AutoModelForCTC.from_pretrained(\"ntu-spml/distilhubert\").to(\"cuda\")\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor)\n",
    "\n",
    "# ========== 5. Local Training ==========\n",
    "\n",
    "def local_finetune(model, dataset, processor, collator, output_dir):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=10,\n",
    "        save_steps=5000,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model.state_dict()\n",
    "\n",
    "# ========== 6. FedAvg Aggregation ==========\n",
    "\n",
    "def fed_avg(state_dicts: List[Dict]):\n",
    "    avg_dict = copy.deepcopy(state_dicts[0])\n",
    "    for key in avg_dict:\n",
    "        for i in range(1, len(state_dicts)):\n",
    "            avg_dict[key] += state_dicts[i][key]\n",
    "        avg_dict[key] = avg_dict[key] / len(state_dicts)\n",
    "    return avg_dict\n",
    "\n",
    "# ========== 7. Federated Learning Loop ==========\n",
    "\n",
    "global_model = copy.deepcopy(base_model)\n",
    "\n",
    "for round_num in range(3):\n",
    "    print(f\"Round {round_num + 1}\")\n",
    "    weights = []\n",
    "    for i, client_data in enumerate(client_datasets):\n",
    "        print(f\"  Client {i+1}\")\n",
    "        local_model = copy.deepcopy(global_model)\n",
    "        state = local_finetune(local_model, client_data, processor, data_collator, f\"./client{i}_round{round_num}\")\n",
    "        weights.append(state)\n",
    "    avg_weights = fed_avg(weights)\n",
    "    global_model.load_state_dict(avg_weights)\n",
    "\n",
    "# ========== 8. Save and Evaluate ==========\n",
    "\n",
    "global_model.save_pretrained(\"./federated_distilhubert_asr\")\n",
    "processor.save_pretrained(\"./federated_distilhubert_asr\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
